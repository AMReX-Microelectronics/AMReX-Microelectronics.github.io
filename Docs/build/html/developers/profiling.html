<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Profiling the Code &mdash; MicroEleX 1.0 documentation</title>
      <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=fa44fd50" />
      <link rel="stylesheet" type="text/css" href="../_static/css/theme.css?v=19f00094" />
      <link rel="stylesheet" type="text/css" href="../_static/design-style.1e8bd061cd6da7fc9cf755528e8ffc24.min.css?v=0a3b3ea7" />

  
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script src="../_static/jquery.js?v=5d32c60e"></script>
        <script src="../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
        <script src="../_static/documentation_options.js?v=f2a433a1"></script>
        <script src="../_static/doctools.js?v=9a2dae69"></script>
        <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
        <script src="../_static/design-tabs.js?v=36754332"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../index.html" class="icon icon-home">
            MicroEleX
          </a>
              <div class="version">
                1.0
              </div>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">INSTALLATION</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../install/artemis.html">ARTEMIS</a></li>
<li class="toctree-l1"><a class="reference internal" href="../install/eleqtroneX.html">ELEQTRONeX</a></li>
<li class="toctree-l1"><a class="reference internal" href="../install/ferrox.html">FerroX</a></li>
<li class="toctree-l1"><a class="reference internal" href="../install/magnex.html">MagneX</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">USAGE</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../usage/run_artemis.html">Run ARTEMIS</a></li>
<li class="toctree-l1"><a class="reference internal" href="../usage/run_eleqtronex.html">Run ELEQTRONeX</a></li>
<li class="toctree-l1"><a class="reference internal" href="../usage/run_ferrox.html">Run FerroX</a></li>
<li class="toctree-l1"><a class="reference internal" href="../usage/run_magnex.html">Run MagneX</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">DATA ANALYSIS</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../dataanalysis/formats.html">Output Formats and Visualization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../dataanalysis/yt.html">yt-project</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">EPILOGUE</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../acknowledgements.html">Funding and Acknowledgements</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">MicroEleX</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">Profiling the Code</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/developers/profiling.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="profiling-the-code">
<span id="developers-profiling"></span><h1>Profiling the Code<a class="headerlink" href="#profiling-the-code" title="Link to this heading"></a></h1>
<p>Profiling allows us to find the bottle-necks of the code as it is currently implemented.
Bottle-necks are the parts of the code that may delay the simulation, making it more computationally expensive.
Once found, we can update the related code sections and improve its efficiency.
Profiling tools can also be used to check how load balanced the simulation is, i.e. if the work is well distributed across all MPI ranks used.
Load balancing can be activated in WarpX by setting input parameters, see the <a class="reference internal" href="../usage/parameters.html#running-cpp-parameters-parallelization"><span class="std std-ref">parallelization input parameter section</span></a>.</p>
<section id="amrex-s-tiny-profiler">
<span id="developers-profiling-tiny-profiler"></span><h2>AMReX’s Tiny Profiler<a class="headerlink" href="#amrex-s-tiny-profiler" title="Link to this heading"></a></h2>
<p>By default, WarpX uses the AMReX baseline tool, the TINYPROFILER, to evaluate the time information for different parts of the code (functions) between the different MPI ranks.
The results, timers, are stored into four tables in the standard output, stdout, that are located below the simulation steps information and above the warnings regarding unused input file parameters (if there were any).</p>
<p>The timers are displayed in tables for which the columns correspond to:</p>
<ul class="simple">
<li><p>name of the function</p></li>
<li><p>number of times it is called in total</p></li>
<li><p>minimum of time spent exclusively/inclusively in it, between all ranks</p></li>
<li><p>average of time, between all ranks</p></li>
<li><p>maximum time, between all ranks</p></li>
<li><p>maximum percentage of time spent, across all ranks</p></li>
</ul>
<p>If the simulation is well load balanced the minimum, average and maximum times should be identical.</p>
<p>The top two tables refer to the complete simulation information.
The bottom two are related to the Evolve() section of the code (where each time step is computed).</p>
<p>Each set of two timers show the exclusive, top, and inclusive, bottom, information depending on whether the time spent in nested sections of the codes are included.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>When creating performance-related issues on the WarpX GitHub repo, please include Tiny Profiler tables (besides the usual issue description, input file and submission script), or (even better) the whole standard output.</p>
</div>
<p>For more detailed information please visit the <a class="reference external" href="https://amrex-codes.github.io/amrex/docs_html/AMReX_Profiling_Tools_Chapter.html">AMReX profiling documentation</a>.
There is a script located <a class="reference external" href="https://github.com/AMReX-Codes/amrex/tree/development/Tools/TinyProfileParser">here</a> that parses the Tiny Profiler output and generates a JSON file that can be used with <a class="reference external" href="https://hatchet.readthedocs.io/en/latest/">Hatchet</a> in order to analyze performance.</p>
</section>
<section id="amrex-s-full-profiler">
<h2>AMReX’s Full Profiler<a class="headerlink" href="#amrex-s-full-profiler" title="Link to this heading"></a></h2>
<p>The Tiny Profiler provides a summary across all MPI ranks. However, when analyzing
load-balancing, it can be useful to have more detailed information about the
behavior of <em>each</em> individual MPI rank. The workflow for doing so is the following:</p>
<ul>
<li><p>Compile WarpX with full profiler support:</p>
<blockquote>
<div><div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>cmake<span class="w"> </span>-S<span class="w"> </span>.<span class="w"> </span>-B<span class="w"> </span>build<span class="w"> </span>-DAMReX_BASE_PROFILE<span class="o">=</span>YES<span class="w"> </span>-DAMReX_TRACE_PROFILE<span class="o">=</span>YES<span class="w">  </span>-DAMReX_COMM_PROFILE<span class="o">=</span>YES<span class="w"> </span>-DAMReX_TINY_PROFILE<span class="o">=</span>OFF
cmake<span class="w"> </span>--build<span class="w"> </span>build<span class="w"> </span>-j<span class="w"> </span><span class="m">4</span>
</pre></div>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Please note that the <a class="reference external" href="https://amrex-codes.github.io/amrex/docs_html/BuildingAMReX.html#customization-options">AMReX build options</a> for <code class="docutils literal notranslate"><span class="pre">AMReX_TINY_PROFILE</span></code> (our default: <code class="docutils literal notranslate"><span class="pre">ON</span></code>) and full profiling traces via <code class="docutils literal notranslate"><span class="pre">AMReX_BASE_PROFILE</span></code> are mutually exclusive.
Further tracing options are sub-options of <code class="docutils literal notranslate"><span class="pre">AMReX_BASE_PROFILE</span></code>.</p>
<p>To turn on the tiny profiler again, remove the <code class="docutils literal notranslate"><span class="pre">build</span></code> directory or turn off <code class="docutils literal notranslate"><span class="pre">AMReX_BASE_PROFILE</span></code> again:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>cmake<span class="w"> </span>-S<span class="w"> </span>.<span class="w"> </span>-B<span class="w"> </span>build<span class="w"> </span>-DAMReX_BASE_PROFILE<span class="o">=</span>OFF<span class="w"> </span>-DAMReX_TINY_PROFILE<span class="o">=</span>ON
</pre></div>
</div>
</div>
</div></blockquote>
</li>
<li><p>Run the simulation to be profiled. Note that the WarpX executable will create
a new folder <cite>bl_prof</cite>, which contains the profiling data.</p>
<blockquote>
<div><div class="admonition note">
<p class="admonition-title">Note</p>
<p>When using the full profiler, it is usually useful to profile only
a few PIC iterations (e.g. 10-20 PIC iterations), in order to improve
readability. If the interesting PIC iterations occur only late in a
simulation, you can run the first part of the simulation without
profiling, the create a checkpoint, and then restart the simulation for
10-20 steps with the full profiler on.</p>
</div>
</div></blockquote>
</li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The next steps can be done on a local computer (even if
the simulation itself ran on an HPC cluster). In this
case, simply copy the folder <cite>bl_prof</cite> to your local computer.</p>
</div>
<ul>
<li><p>In order, to visualize the profiling data, install <cite>amrvis</cite> using <cite>spack</cite>:</p>
<blockquote>
<div><div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>spack<span class="w"> </span>install<span class="w"> </span>amrvis<span class="w"> </span><span class="nv">dims</span><span class="o">=</span><span class="m">2</span><span class="w"> </span>+profiling
</pre></div>
</div>
</div></blockquote>
</li>
<li><p>Then create timeline database from the <cite>bl_prof</cite> data and open it:</p>
<blockquote>
<div><blockquote>
<div><div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>&lt;amrvis-executable&gt;<span class="w"> </span>-timelinepf<span class="w"> </span>bl_prof/
&lt;amrvis-executable&gt;<span class="w"> </span>pltTimeline/
</pre></div>
</div>
</div></blockquote>
<p>In the above, <cite>&lt;amrvis-executable&gt;</cite> should be replaced by the actual of your
<cite>amrvis</cite> executable, which can be found starting to type <cite>amrvis</cite> and then
using Tab completion, in a Terminal.</p>
</div></blockquote>
</li>
<li><dl class="simple">
<dt>This will pop-up a window with the timeline. Here are few guidelines to navigate it:</dt><dd><ul class="simple">
<li><p>Use the horizontal scroller to find the area where the 10-20 PIC steps occur.</p></li>
<li><p>In order to zoom on an area, you can drag and drop with the mouse, and the hit <cite>Ctrl-S</cite> on a keyboard.</p></li>
<li><p>You can directly click on the timeline to see which actual MPI call is being perform. (Note that the colorbar can be misleading.)</p></li>
</ul>
</dd>
</dl>
</li>
</ul>
</section>
<section id="nvidia-nsight-systems">
<span id="developers-profiling-nsight-systems"></span><h2>Nvidia Nsight-Systems<a class="headerlink" href="#nvidia-nsight-systems" title="Link to this heading"></a></h2>
<p><a class="reference external" href="https://developer.nvidia.com/nsight-systems">Vendor homepage</a> and <a class="reference external" href="https://docs.nvidia.com/nsight-systems/">product manual</a>.</p>
<p>Nsight-Systems provides system level profiling data, including CPU and GPU
interactions. It runs quickly, and provides a convenient visualization of
profiling results including NVTX timers.</p>
<section id="perlmutter-example">
<h3>Perlmutter Example<a class="headerlink" href="#perlmutter-example" title="Link to this heading"></a></h3>
<p>Example on how to create traces on a multi-GPU system that uses the Slurm scheduler (e.g., NERSC’s Perlmutter system).
You can either run this on an interactive node or use the Slurm batch script header <a class="reference internal" href="../install/hpc/perlmutter.html#running-cpp-perlmutter-a100-gpus"><span class="std std-ref">documented here</span></a>.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># GPU-aware MPI</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">MPICH_GPU_SUPPORT_ENABLED</span><span class="o">=</span><span class="m">1</span>
<span class="c1"># 1 OpenMP thread</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">OMP_NUM_THREADS</span><span class="o">=</span><span class="m">1</span>

<span class="nb">export</span><span class="w"> </span><span class="nv">TMPDIR</span><span class="o">=</span><span class="s2">&quot;</span><span class="nv">$PWD</span><span class="s2">/tmp&quot;</span>
rm<span class="w"> </span>-rf<span class="w"> </span><span class="si">${</span><span class="nv">TMPDIR</span><span class="si">}</span><span class="w"> </span>profiling*
mkdir<span class="w"> </span>-p<span class="w"> </span><span class="si">${</span><span class="nv">TMPDIR</span><span class="si">}</span>

<span class="c1"># 2021.5.1 (broken: lacks most trace info)</span>
<span class="c1">#NSYS=&quot;/global/common/software/nersc/pm-2021q4/easybuild/software/Nsight-Systems/2021.5.1/bin/nsys&quot;</span>
<span class="c1"># 2021.4.1 (working)</span>
<span class="nv">NSYS</span><span class="o">=</span><span class="s2">&quot;/opt/nvidia/hpc_sdk/Linux_x86_64/21.11/compilers/bin/nsys&quot;</span>

<span class="c1"># record</span>
srun<span class="w"> </span>--ntasks<span class="o">=</span><span class="m">4</span><span class="w"> </span>--gpus<span class="o">=</span><span class="m">4</span><span class="w"> </span>--cpu-bind<span class="o">=</span>cores<span class="w"> </span><span class="se">\</span>
<span class="w">    </span><span class="si">${</span><span class="nv">NSYS</span><span class="si">}</span><span class="w"> </span>profile<span class="w"> </span>-f<span class="w"> </span><span class="nb">true</span><span class="w">               </span><span class="se">\</span>
<span class="w">      </span>-o<span class="w"> </span>profiling_%q<span class="o">{</span>SLURM_TASK_PID<span class="o">}</span><span class="w">     </span><span class="se">\</span>
<span class="w">      </span>-t<span class="w"> </span>mpi,cuda,nvtx,osrt,openmp<span class="w">        </span><span class="se">\</span>
<span class="w">      </span>--mpi-impl<span class="o">=</span>mpich<span class="w">                    </span><span class="se">\</span>
<span class="w">    </span>./warpx.3d.MPI.CUDA.DP.QED<span class="w">            </span><span class="se">\</span>
<span class="w">      </span>inputs_3d<span class="w">                           </span><span class="se">\</span>
<span class="w">        </span>warpx.numprocs<span class="o">=</span><span class="m">1</span><span class="w"> </span><span class="m">1</span><span class="w"> </span><span class="m">4</span><span class="w"> </span>amr.n_cell<span class="o">=</span><span class="m">512</span><span class="w"> </span><span class="m">512</span><span class="w"> </span><span class="m">2048</span><span class="w"> </span><span class="nv">max_step</span><span class="o">=</span><span class="m">10</span>
</pre></div>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>March 23rd, 2022 (INC0182505):
Currently, the environment pre-loads a <code class="docutils literal notranslate"><span class="pre">Nsight-Systems/2021.5.1</span></code> module that ships <code class="docutils literal notranslate"><span class="pre">nsys</span></code> version 2021.5.1.
This version does not record all trace information.
You need to use the one directly shipped with the NVHPC base system, version 2021.4.1, located in <code class="docutils literal notranslate"><span class="pre">/opt/nvidia/hpc_sdk/Linux_x86_64/21.11/compilers/bin/nsys</span></code>.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If everything went well, you will obtain as many output files named <code class="docutils literal notranslate"><span class="pre">profiling_&lt;number&gt;.nsys-rep</span></code> as active MPI ranks.
Each MPI rank’s performance trace can be analyzed with the Nsight System graphical user interface (GUI).
In WarpX, every MPI rank is associated with one GPU, which each creates one trace file.</p>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>The last line of the sbatch file has to match the data of your input files.</p>
</div>
</section>
<section id="summit-example">
<h3>Summit Example<a class="headerlink" href="#summit-example" title="Link to this heading"></a></h3>
<blockquote>
<div><p>Example on how to create traces on a multi-GPU system that uses the <code class="docutils literal notranslate"><span class="pre">jsrun</span></code> scheduler (e.g., <a class="reference external" href="https://docs.olcf.ornl.gov/systems/summit_user_guide.html#optimizing-and-profiling">OLCF’s Summit system</a>):</p>
</div></blockquote>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># nsys: remove old traces</span>
rm<span class="w"> </span>-rf<span class="w"> </span>profiling*<span class="w"> </span>tmp-traces
<span class="c1"># nsys: a location where we can write temporary nsys files to</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">TMPDIR</span><span class="o">=</span><span class="nv">$PWD</span>/tmp-traces
mkdir<span class="w"> </span>-p<span class="w"> </span><span class="nv">$TMPDIR</span>
<span class="c1"># WarpX: one OpenMP thread per MPI rank</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">OMP_NUM_THREADS</span><span class="o">=</span><span class="m">1</span>

<span class="c1"># record</span>
jsrun<span class="w"> </span>-n<span class="w"> </span><span class="m">4</span><span class="w"> </span>-a<span class="w"> </span><span class="m">1</span><span class="w"> </span>-g<span class="w"> </span><span class="m">1</span><span class="w"> </span>-c<span class="w"> </span><span class="m">7</span><span class="w"> </span>--bind<span class="o">=</span>packed:<span class="nv">$OMP_NUM_THREADS</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>nsys<span class="w"> </span>profile<span class="w"> </span>-f<span class="w"> </span><span class="nb">true</span><span class="w"> </span><span class="se">\</span>
<span class="w">      </span>-o<span class="w"> </span>profiling_%p<span class="w"> </span><span class="se">\</span>
<span class="w">      </span>-t<span class="w"> </span>mpi,cuda,nvtx,osrt,openmp<span class="w">   </span><span class="se">\</span>
<span class="w">      </span>--mpi-impl<span class="o">=</span>openmpi<span class="w">             </span><span class="se">\</span>
<span class="w">    </span>./warpx.3d.MPI.CUDA.DP.QED<span class="w"> </span>inputs_3d<span class="w"> </span><span class="se">\</span>
<span class="w">      </span>warpx.numprocs<span class="o">=</span><span class="m">1</span><span class="w"> </span><span class="m">1</span><span class="w"> </span><span class="m">4</span><span class="w"> </span>amr.n_cell<span class="o">=</span><span class="m">512</span><span class="w"> </span><span class="m">512</span><span class="w"> </span><span class="m">2048</span><span class="w"> </span><span class="nv">max_step</span><span class="o">=</span><span class="m">10</span>
</pre></div>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Sep 10th, 2021 (OLCFHELP-3580):
The Nsight-Compute (<code class="docutils literal notranslate"><span class="pre">nsys</span></code>) version installed on Summit does not record details of GPU kernels.
This is reported to Nvidia and OLCF.</p>
</div>
</section>
<section id="details">
<h3>Details<a class="headerlink" href="#details" title="Link to this heading"></a></h3>
<p>In these examples, the individual lines for recording a trace profile are:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">srun</span></code>: execute multi-GPU runs with <code class="docutils literal notranslate"><span class="pre">srun</span></code> (Slurm’s <code class="docutils literal notranslate"><span class="pre">mpiexec</span></code> wrapper), here for four GPUs</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">-f</span> <span class="pre">true</span></code> overwrite previously written trace profiles</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">-o</span></code>: record one profile file per MPI rank (per GPU); if you run <code class="docutils literal notranslate"><span class="pre">mpiexec</span></code>/<code class="docutils literal notranslate"><span class="pre">mpirun</span></code> with OpenMPI directly, replace <code class="docutils literal notranslate"><span class="pre">SLURM_TASK_PID</span></code> with <code class="docutils literal notranslate"><span class="pre">OMPI_COMM_WORLD_RANK</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">-t</span></code>: select a couple of APIs to trace</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">--mpi--impl</span></code>: optional, hint the MPI flavor</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">./warpx...</span></code>: select the WarpX executable and a good inputs file</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">warpx.numprocs=...</span></code>: make the run short, reasonably small, and run only a few steps</p></li>
</ul>
<p>Now open the created trace files (per rank) in the Nsight-Systems GUI.
This can be done on another system than the one that recorded the traces.
For example, if you record on a cluster and open the analysis GUI on your laptop, it is recommended to make sure that versions of Nsight-Systems match on the remote and local system.</p>
</section>
</section>
<section id="nvidia-nsight-compute">
<h2>Nvidia Nsight-Compute<a class="headerlink" href="#nvidia-nsight-compute" title="Link to this heading"></a></h2>
<p><a class="reference external" href="https://developer.nvidia.com/nsight-compute">Vendor homepage</a> and <a class="reference external" href="https://docs.nvidia.com/nsight-compute/">product manual</a>.</p>
<p>Nsight-Compute captures fine grained information at the kernel level
concerning resource utilization. By default, it collects a lot of data and runs
slowly (can be a few minutes per step), but provides detailed information about
occupancy, and memory bandwidth for a kernel.</p>
<section id="example">
<h3>Example<a class="headerlink" href="#example" title="Link to this heading"></a></h3>
<p>Example of how to create traces on a single-GPU system. A jobscript for
Perlmutter is shown, but the <cite>SBATCH</cite> headers are not strictly necessary as the
command only profiles a single process. This can also be run on an interactive
node, or without a workload management system.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="ch">#!/bin/bash -l</span>
<span class="c1">#SBATCH -t 00:30:00</span>
<span class="c1">#SBATCH -N 1</span>
<span class="c1">#SBATCH -J ncuProfiling</span>
<span class="c1">#SBATCH -A &lt;your account&gt;</span>
<span class="c1">#SBATCH -q regular</span>
<span class="c1">#SBATCH -C gpu</span>
<span class="c1">#SBATCH --ntasks-per-node=1</span>
<span class="c1">#SBATCH --gpus-per-task=1</span>
<span class="c1">#SBATCH --gpu-bind=map_gpu:0</span>
<span class="c1">#SBATCH --mail-user=&lt;email&gt;</span>
<span class="c1">#SBATCH --mail-type=ALL</span>

<span class="c1"># record</span>
dcgmi<span class="w"> </span>profile<span class="w"> </span>--pause
ncu<span class="w"> </span>-f<span class="w"> </span>-o<span class="w"> </span>out<span class="w"> </span><span class="se">\</span>
--target-processes<span class="w"> </span>all<span class="w"> </span><span class="se">\</span>
--set<span class="w"> </span>detailed<span class="w"> </span><span class="se">\</span>
--nvtx<span class="w"> </span>--nvtx-include<span class="o">=</span><span class="s2">&quot;WarpXParticleContainer::DepositCurrent::CurrentDeposition/&quot;</span><span class="w"> </span><span class="se">\</span>
./warpx<span class="w"> </span>input<span class="w"> </span><span class="nv">max_step</span><span class="o">=</span><span class="m">1</span><span class="w"> </span><span class="se">\</span>
<span class="p">&amp;</span>&gt;<span class="w"> </span>warpxOut.txt
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>To collect full statistics, Nsight-Compute reruns kernels,
temporarilly saving device memory in host memory. This makes it
slower than Nsight-Systems, so the provided script profiles only a single
step of a single process. This is generally enough to extract relevant
information.</p>
</div>
</section>
<section id="id1">
<h3>Details<a class="headerlink" href="#id1" title="Link to this heading"></a></h3>
<p>In the example above, the individual lines for recording a trace profile are:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">dcgmi</span> <span class="pre">profile</span> <span class="pre">--pause</span></code> other profiling tools can’t be collecting data,
<a class="reference external" href="https://forums.developer.nvidia.com/t/profiling-failed-because-a-driver-resource-was-unavailable/205435">see this Q&amp;A</a>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">-f</span></code> overwrite previously written trace profiles.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">-o</span></code>: output file for profiling.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">--target-processes</span> <span class="pre">all</span></code>: required for multiprocess code.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">--set</span> <span class="pre">detailed</span></code>: controls what profiling data is collected. If only
interested in a few things, this can improve profiling speed.
<code class="docutils literal notranslate"><span class="pre">detailed</span></code> gets pretty much everything.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">--nvtx</span></code>: collects NVTX data. See note.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">--nvtx-include</span></code>: tells the profiler to only profile the given sections.
You can also use <code class="docutils literal notranslate"><span class="pre">-k</span></code> to profile only a given kernel.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">./warpx...</span></code>: select the WarpX executable and a good inputs file.</p></li>
</ul>
<p>Now open the created trace file in the Nsight-Compute GUI. As with
Nsight-Systems,
this can be done on another system than the one that recorded the traces.
For example, if you record on a cluster and open the analysis GUI on your laptop, it is recommended to make sure that versions of Nsight-Compute match on the remote and local system.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>nvtx-include syntax is very particular. The trailing / in the example is
significant. For full information, see the Nvidia’s documentation on <a class="reference external" href="https://docs.nvidia.com/nsight-compute/NsightComputeCli/index.html#nvtx-filtering">NVTX filtering</a> .</p>
</div>
</section>
</section>
</section>


           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2017-2024, MicroEleX collaboration.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>